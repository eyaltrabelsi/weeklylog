# weeklylog 01.12.2016 - 08.12.2016

## Articles
- [Beginners Guide to Topic Modeling in Python](https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/) `topic modeling` `favorite` `101` `nlp` `python`
- [Steps for effective text data cleaning (with case study using Python)](https://www.analyticsvidhya.com/blog/2014/11/text-data-cleaning-steps-python/?utm_content=buffer5bbac&utm_medium=social&utm_source=facebook.com&utm_campaign=buffer) `python` `data munging` `data engineer` `101`
- [3 Must Know Analytical Concepts For Every Professional / Fresher in Analytics](https://www.analyticsvidhya.com/blog/2016/07/3-analytical-concepts-every-professional-fresher-in-analytics/) `analytics concepts` `101`
- [How Agari Uses Airbnb's Airflow As A Smarter Cron](http://highscalability.com/blog/2015/9/3/how-agari-uses-airbnbs-airflow-as-a-smarter-cron.html) `airflow` `agari` 
- [Updateable Views in PostgreSQL 9.1 using INSTEAD OF Trigger](https://vibhorkumar.wordpress.com/2011/10/28/instead-of-trigger/) `updateable views` `postgres`
- [14.4. Populating a Database](https://www.postgresql.org/docs/current/static/populate.html) `postgress` `data engineer`
- [Winning Strategies for ML Competitions from Past Winners](https://www.analyticsvidhya.com/blog/2016/10/winning-strategies-for-ml-competitions-from-past-winners/) `ml` `favorite` `best practices`
- [Is word2vec basically an autoencoder?](https://www.quora.com/Is-word2vec-basically-an-autoencoder) `ml` `autoencoder` `word2vec`
- [Why great software engineers stream their projects live](https://www.techinasia.com/talk/great-software-engineers-stream-projects-live) `live stream` `software engineer`
- [How to share data with a statistician](https://github.com/jtleek/datasharing) `favorite` `communication` `statistic`
 
## Videos
- [Sadayuki Furuhashi | Fighting Against Chaotically Separated Values with Embulk](https://www.youtube.com/watch?v=ocuez7Mikh4&list=PLGVZCDnMOq0q9qZsHfueAJsMiCbtvlvWr&index=17) `favorite` `data engineering` `data load` `embulk` `data flow`  
- [Sanghamitra Deb | Creating Knowledgebases from unstructured text](https://www.youtube.com/watch?v=gf0_2VeJaLU&list=PLGVZCDnMOq0q9qZsHfueAJsMiCbtvlvWr&index=34) `favorite` `semi-supervised learning` `data science` 
- [Cathy Deng | Machine learning techniques for data cleaning](https://www.youtube.com/watch?v=bzclXbqmsQg&list=PLGVZCDnMOq0pMUpVnQ8h5byVzIJNZCo2S&index=5) `favorite` `data engineering` `data science` `data munging` 

## Tips
- **statistics variables variables**:
    - types:
        * Continuous variables are anything measured on a quantitative scale that could be any fractional number. An example would be something like weight measured in kg.
        * Ordinal data are data that have a fixed, small (< 100) number of levels but are ordered. This could be for example survey responses where the choices are: poor, fair, good.
        * Categorical data are data where there are multiple categories, but they aren't ordered. One example would be sex: male or female. This coding is attractive because it is self-documenting.
        * Missing data are data that are unobserved and you don't know the mechanism. You should code missing values as NA.
        * Censored data are data where you know the missingness mechanism on some level. Common examples are a measurement being below a detection limit or a patient being lost to follow-up. They should also be coded as NA when you don't have the data. But you should also add a new column to your tidy data called, "VariableNameCensored" which should have values of TRUE if censored and FALSE if not. In the code book you should explain why those values are missing. It is absolutely critical to report to the analyst if there is a reason you know about that some of the data are missing.
    - In general, try to avoid coding categorical or ordinal variables as numbers. When you enter the value for sex in the tidy data, it should be "male" or "female". The ordinal values in the data set should be "poor", "fair", and "good" not 1, 2 ,3. This will avoid potential mixups about which direction effects go and will help identify coding errors.
- **To facilitate the most efficient and timely analysis this is the information you should pass to a statistician**:
    - The raw data , It is critical that you include the rawest form of the data that you have access to. This ensures that data provenance can be maintained throughout the workflow, You know the raw data are in the right format if you:
        * Ran no software on the data
        * Did not modify any of the data values
        * You did not remove any data from the data set
        * You did not summarize the data in any way
    - A tidy data set
    - A code book describing each variable and its values in the tidy data set , he measurements you calculate will need to be described in more detail than you can or should sneak into the spreadsheet. The code book contains this information. At minimum it should contain:
        * Information about the variables (including units!) in the data set not contained in the tidy data
        * Information about the summary choices you made
        * Information about the experimental study design you used
- **Practical advice for analysis of large, complex data sets**: 
    - Technical:
        * Look at your distributions, While we typically use summary metrics (means, median, standard deviation, etc.) to communicate about distributions, you should usually be looking at a much richer representation of the distribution.
          Something like histograms, CDFs, Q-Q plots, etc. will allow you to see if there are important interesting features of the data such as multi-modal behavior or a significant class of outliers that you need to decide how to summarize.
        * Consider the outliers, You should look at the outliers in your data. They can be canaries in the coal mine for more fundamental problems with your analysis. It’s fine to exclude them from your data or to lump them together into an “Unusual” category, but you should make sure you know why data ended up in that category.
          For example, looking at the queries with the lowest click-through rate (CTR) may reveal clicks on elements in the user interface that you are failing to count. Looking at queries with the highest CTR may reveal clicks you should not be counting. On the other hand, some outliers you will never be able to explain so you need to be careful in how much time you devote this.
        * Report noise/confidence,  First and foremost, we must be aware that randomness exists and will fool us. If you aren’t careful, you will find patterns in the noise. Every estimator that you produce should have a notion of your confidence in this estimate attached to it.
          Sometimes this will be more formal and precise (through techniques such as confidence intervals or credible intervals for estimators, and p-values or Bayes factors for conclusions) and other times you will be more loose.
          For example if a colleague asks you how many queries about frogs we get on Mondays, you might do a quick analysis looking and a couple of Mondays and report “usually something between 10 and 12 million” (not real numbers). 
        * Look at examples,  Anytime you are producing new analysis code, you need to look at examples of the underlying data and how your code is interpreting those examples. Your analysis is removing lots of features from the underlying data to produce useful summaries. By looking at the full complexity of individual examples, you can gain confidence that your summarization is reasonable.
          You should be doing stratified sampling to look at a good sample across the distribution of values so you are not too focussed on the most common cases.
        * Slice your data, Slicing means to separate your data into subgroups and look at the values of your metrics in those subgroups separately. In analysis of web traffic, we commonly slice along dimensions like mobile vs. desktop, browser, locale, etc.
          If the underlying phenomenon is likely to work differently across subgroups, you must slice the data to see if it is. Even if you do not expect a slice to matter, looking at a few slices for internal consistency gives you greater confidence that you are measuring the right thing.
          In some cases, a particular slice may have bad data, a broken experience, or in some way be fundamentally different. Anytime you are slicing your data to compare two groups beware when the amount of data in a slice is different across the groups you are comparing.
        * Consider practical significance, With a large volume of data, it can be tempting to focus solely on statistical significance or to hone in on the details of every bit of data. But you need to ask yourself, “Even if it is true that value X is 0.1% more than value Y, does it matter?” This can be especially important if you are unable to understand/categorize part of your data. If you are unable to make sense of some user agents strings in our logs, whether it’s 0.1% of 10% makes a big difference in how much you should investigate those cases.
          On the flip side, you sometimes have a small volume of data. Many changes will not look statistically significant but that is different than claiming it is “neutral”. You must ask yourself “How likely is it that there is still a practically significant change”?
        * Check for consistency over time ,One particular slicing you should almost always employ is to slice by units of time . This is because many disturbances to underlying data happen as our systems evolve over time. Typically the initial version of a feature or the initial data collection will be checked carefully, but it is not uncommon for something to break along the way. Just because a particular day or set of days is an outlier does not mean you should discard it. Use the data as a hook to find a causal reason for that day being different before you discard it.
          The other benefit of looking at day over day data is it gives you a sense of the variation in the data that would eventually lead to confidence intervals or claims of statistical significance. This should not generally replace rigorous confidence interval calculation, but often with large changes you can see they will be statistically significant just from the day-over-day graphs.
     - Process:
        * Check vital signs, Before actually answering the question you are interested in, you need to check for a lot of other things that may not be related to what you are interested in but may be useful in later analysis or indicate problems in the data.
        * Measure twice, or more, Especially if you are trying to capture a new phenomenon, try to measure the same underlying thing in multiple ways. Then, check to see if these multiple measurements are consistent. By using multiple measurements, you can identify bugs in measurement or logging code, unexpected features of the underlying data, or filtering steps that are important. It’s even better if you can use different data sources for the measurements.
        * Check for reproducibility, Both slicing and consistency over time are particular examples of checking for reproducibility. If a phenomenon is important and meaningful, you should see it across different user populations and time. But reproducibility means more than this as well. If you are building models of the data, you want those models to be stable across small perturbations in the underlying data. 
        * Standard first, custom second This is a variant of checking for what shouldn’t change. Especially when looking at new features and new data, it’s tempting to jump right into the metrics that are novel or special for this new feature. But you should always look at standard metrics first, even if you expect them to change.
          If your new, custom metrics don’t make sense with your standard metrics, your new, custom metrics are likely wrong.
        * Check for consistency with past measurements Often you will be calculating a metric that is similar to things that have been counted in the past. You should compare your metrics to metrics reported in the past.
          Even if these measurements are on different user populations and there i conflict assume that you are wrong until you can fully convince yourself. Most surprising data will turn out to be a error, not a fabulous new insight.
        * Exploratory analysis benefits from end to end iterations, you should strive to get as many iterations of the whole analysis as possible. Therefore, your initial focus should not be on perfection but on getting something reasonable all the way through. 
          Leave notes for yourself and acknowledge things like filtering steps and data records that you can’t parse/understand, but trying to get rid of all of them is a waste of time at the beginning of exploratory analysis.
        * Confirm expt/data collection setup, Before looking at any data, make sure you understand the experiment and data collection setup. Communicating precisely between the experimentalist and the analyst is a big challenge.
          You may spot unusual or bad configurations or population restrictions (such as valid data only for a particular browser).
        * Make hypotheses and look for evidence, exploratory data analysis for a complex problem is iterative. You will discover anomalies, trends, or other features of the data. Naturally, you will make hypotheses to explain this data. It’s essential that you don’t just make a hypothesis and proclaim it to be true. Look for evidence (inside or outside the data) to confirm/deny this theory. 
          Good data analysis will have a story to tell. To make sure it’s the right story, you need to tell the story to yourself, predict what else you should see in the data if that hypothesis is true, then look for evidence that it’s wrong. One way of doing this is to ask yourself, “What experiments would I run that would validate/invalidate the story I am telling?” Even if you don’t/can’t do these experiments, it may give you ideas on how to validate with the data that you do have.     
        * Separate Validation, Description, and Evaluation, I think about about exploratory data analysis as having 3 interrelated stages:
           i. Validation or Initial Data Analysis: Do I believe data is self-consistent, that the data was collected correctly, and that data represents what I think it does? This often goes under the name of “sanity checking”
           ii. Description: What’s the objective interpretation of this data? Description should be things that everyone can agree on from the data.
           iii. Evaluation: Given the description, does the data tell us that something good is happening for the user, for Google, for the world? Evaluation is likely to have much more debate because you imbuing meaning and value to the data. 
     - Social: 
        * Data analysis starts with questions, If you take the time to formulate your needs as questions or hypotheses, it will go a long way towards making sure that you are gathering the data you should be gathering and that you are thinking about the possible gaps in the data. Of course, the questions you ask can and should evolve as you look at the data. not data or a technique.
          Further, you have to avoid the trap of finding some favorite technique and then only finding the parts of problems that this technique works on. 
        * Acknowledge and count your filtering,  one should Acknowledge and clearly specify what filtering you are doing and Count how much is being filtered at each of your steps
        * Educate your consumers You will often be presenting your analysis and results to people who are not data experts. Part of your job is to educate them on how to interpret and draw conclusions from your data. This runs the gamut from making sure they understand confidence intervals to why certain measurements are unreliable in your domain .
          You are responsible for providing the context and a full picture of the data and not just the number a consumer asked for.
        * Be both skeptic and champion ,  You will hopefully find some interesting phenomena in the data you look at. When you have an interesting phenomenon you should ask both “What other data could I gather to show how awesome this is?” and “What could I find that would invalidate this?”.
        * Share with peers first, external consumers second, A skilled peer reviewer can provide qualitatively different feedback and sanity-checking than the consumers of your data can, especially since consumers generally have an outcome they want to get.
          Ideally, you will have a peer that knows something about the data you are looking at, but even a peer with just experience looking at data in general is extremely valuable. 
        * Expect and accept ignorance and mistakes, There are many limits to what we can learn from data. Nate Silver makes a strong case in The Signal and the Noise that only by admitting the limits of our certainty can we make advances in better prediction.
          Admitting ignorance is a strength but it is not usually immediately rewarded.
